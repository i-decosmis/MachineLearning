{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "The code reads data from the diabetes.csv file in the Dataset folder and loads it into a pandas DataFrame. Next, it creates a SimpleImputer object with the imputation strategy set to \"mean\" and the missing value set to 0. The imputer is then used to impute missing values in all columns of the dataset except \"Outcome\".\n",
    "\n",
    "After imputation, the code separates input features from output class and increases the relevance of significant features by multiplying them by 2. Next, it creates a `MinMaxScaler` object and fits it to the input features. The input features are then transformed using the scaler.\n",
    "\n",
    "We then divide the data into training, validation and test sets using the `train_test_split` function. We also calculate class weights using the `compute_class_weight` function and save class weights in a dictionary.\n",
    "\n",
    "Finally, we also save processed data in .npy files in the Pre_Processed_Data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76607143 1.43959732]\n",
      "train_features: 429\n",
      "test_features: 231\n",
      "val_features: 108\n",
      "total features: 768\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Reading data from dataset\n",
    "data = pd.read_csv('Dataset/diabetes.csv')\n",
    "\n",
    "# Create a SimpleImputer object with the imputation strategy set to \"mean\" and the missing value set to 0\n",
    "imputer = SimpleImputer(strategy='mean', missing_values=0)\n",
    "\n",
    "# Impute missing values in all columns of the dataset except \"Outcome\"\n",
    "data.iloc[:, :-1] = imputer.fit_transform(data.iloc[:, :-1])\n",
    "\n",
    "# Separate input features from output class\n",
    "features = data.drop(['Outcome'], axis=1)\n",
    "labels = data['Outcome']\n",
    "\n",
    "# Increasing the relevance of significant features\n",
    "significant_features = ['Pregnancies', 'Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "features[significant_features] *= 2\n",
    "\n",
    "# Create a MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Adapting scaler on input features\n",
    "scaler.fit(features)\n",
    "\n",
    "# Transforming input features with scaler\n",
    "features_normalized = scaler.transform(features)\n",
    "\n",
    "# Divide data into training, validation and test sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features_normalized, labels, test_size=0.3, random_state=42)\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weighting\n",
    "class_labels = np.unique(train_labels)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=train_labels)\n",
    "# Create a dictionary to store class weighting\n",
    "class_weights_dict = dict(zip(class_labels, class_weights))\n",
    "\n",
    "# Saving datas\n",
    "np.save('Pre_Processed_Data/class_weights.npy', class_weights_dict)\n",
    "np.save('Pre_Processed_Data/train_features.npy', train_features)\n",
    "np.save('Pre_Processed_Data/train_labels.npy', train_labels)\n",
    "np.save('Pre_Processed_Data/val_features.npy', val_features)\n",
    "np.save('Pre_Processed_Data/val_labels.npy', val_labels)\n",
    "np.save('Pre_Processed_Data/test_features.npy', test_features)\n",
    "np.save('Pre_Processed_Data/test_labels.npy', test_labels)\n",
    "\n",
    "print(\"train_features:\", len(train_features))\n",
    "print(\"test_features:\", len(test_features))\n",
    "print(\"val_features:\", len(val_features))\n",
    "print(\"total features:\",len(features_normalized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
